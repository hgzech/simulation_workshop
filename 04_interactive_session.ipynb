{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp interactive_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Define the experiment.\n",
    "First we define how the experiment works.  Here, we will simulate a two-armed bandit experiment.  In the two-armed bandit task participants repeatedly choose between two stimuli. Each stimulus has a certain <b>reward probability</b>.  Participants complete several <b>trials</b> and their goal is to maximize <b>rewards</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Define the number of trials and reward probabilities.\n",
    "First, we have to decide on the number of trials and the reward probabilities for each stimulus.  Store the number of trials in a variable `T` and store the reward probabilities (one for each of the two stimuli) in variable `mu`.\n",
    "> Note: You can define these parameters however you want.  In the Collins paper there were 100 trials and the reward probabilities were .2 for one stimulus and .8 for the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "mu = [.2, .8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Write the reward function.\n",
    "The reward function allocates reward stochastically based on reward probabilities. This function uses an action and the reward probabilities as inputs and returns either a reward of 1 or no reward (0).  \n",
    "\n",
    "It achieves this by generating a random number from a uniform distribution ranging from 0 and 1 (in Python you can do this using `np.random.random()`).\n",
    "\n",
    "Next, it compares the reward probability associated with the chosen action `mu[action]` to that number.  If it is lower or equal `<=` to the reward probability it returns 1 (reward) otherwise it returns 0 (no reward).\n",
    "\n",
    "> Note, in python you implement functions by writing:\n",
    "```\n",
    "def allocate_reward(action, mu): # choice and mu are your inputs\n",
    "    ...\n",
    "    return reward # reward is your return value\n",
    "```\n",
    "> You implement if statements, by writing:\n",
    "```\n",
    "if ...:\n",
    "    ...\n",
    "else:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_reward(action, mu):\n",
    "    if np.random.random() < mu[action]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Set up your first behavioral model (Rescorla-Wagner).\n",
    "The Rescorla-Wagner model consists of a softmax choice rule and a Q-learning update algorithm.  Initially, the model does not know which action is the best choice, so both actions should have the same Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize your Q-values\n",
    "In this step you define the initial Q-values. Store them in a variable `Q`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We will need to run some calculations with these Q-values.  To allow for this you'll have to place your Q-values into a numpy array `np.array([...,...])`.  The initial values should not matter much, but Collins et al. set them to .5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement the softmax function.\n",
    "The softmax function determines <b>choice probabilities</b> based on Q-values using the following formula:\n",
    "\n",
    "$p_a = \\dfrac{exp(Q_{at}*beta)} {\\Sigma{exp(Q_{at}*beta)}}$\n",
    "\n",
    "As you can see, this function also introduces our first participant parameter the<b> inverse temperature (beta)</b>.  The larger beta the more deterministic (less random) a person's choices are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bonus Question: What else does this function do, next to introducing beta?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Define beta\n",
    "First define beta and store it in the variable `beta`.  Here you can pick any positive value (we chose 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bonus Question: Why does beta need to be positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Implement the softmax function.\n",
    "> Note, remember in python you implement functions by writing:\n",
    "```\n",
    "def softmax(Q, beta): # Q and beta are your inputs\n",
    "    ...\n",
    "    return p # p is your return value\n",
    "```\n",
    "\n",
    "> To exponentiate an array you use `np.exp()` and to calculate the sum of an array, you use `np.sum()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Q, beta):\n",
    "    p = np.exp(Q*beta) / sum(np.exp(Q*beta))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write a decision function.\n",
    "This function uses choice probabilities as an input and returns a concrete choice as an output.  It achieves this by first generating a random number from a uniform distribution ranging from 0 and 1 (in Python you can do this using `np.random.random()`).\n",
    "\n",
    "Next, it compares the first choice probability to this number.  If it is lower or equal to the choice probability the chosen action is 0, else the chosen action is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Call this function `choose()`.  You will need an if statement.  Remember, in Python, you define this as:\n",
    "```\n",
    "if ...:\n",
    "    ...\n",
    "else:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(p):\n",
    "    random_number = np.random.random()\n",
    "    if random_number <= p[0]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write the update function\n",
    "The update function updates Q-values based on the reward, according to this formula:\n",
    "$Q_{a,t+1} = Q_{a,t} + \\alpha (r - Q_{a,t})$\n",
    "\n",
    "As you can see, this function introduces another participant parameter: the <b>learning rate (alpha)</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Define alpha\n",
    "Pick a value between 0 and 1 (we chose .2) and store it as `alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2: Write the update function\n",
    "Call this function `update_q_value()` with `Q`, `action`, `reward`, and `alpha` as inputs and `updated_q` and `delta` as outputs.  This function first calculates the prediction error and then updates Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(Q, action, reward, alpha):\n",
    "    delta = reward - Q[action] # prediction error\n",
    "    Q[action] = Q[action] + alpha * delta\n",
    "    return Q, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Putting it all together\n",
    "Congratulations, you created all the components of your simulation function.  Now let's put them all together and simulate some data.  The simulation function (call it `simulate_M3RescorlaWagner_v1`) takes the experiment parameters, `mu` and `T`, as well as the participant parameters `alpha` and `beta` as inputs.\n",
    "\n",
    "> Note: Don't forget to initialize the q-values as we did in step 2.1.\n",
    "\n",
    "In each trial, we simulate an action chosen by the participant, allocate a reward, and update the participant's q-values accordingly.\n",
    "\n",
    "> Note: In Python you write loops like this:\n",
    "```\n",
    "for t in range(T):\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "As outputs it returns the lists (i.e., vectors) `actions`, `rewards`, `Qs`, and `deltas` for each trial.\n",
    "\n",
    "> Note: You create vectors like this `actions = []` and add data to a vector like this: `actions.append(action)`.\n",
    "\n",
    "> Warning: For technical reasons, when you append Q to Qs, you have to append a copy of Q with `Qs.append(Q.copy())`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_M3RescorlaWagner_v1(T, mu, alpha, beta):\n",
    "    Qs = []\n",
    "    deltas = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    Q = np.array([.5, .5])\n",
    "\n",
    "    for t in range(T): \n",
    "        Qs.append(Q.copy())\n",
    "        p = softmax(Q, beta)\n",
    "        action = choose(p)\n",
    "        reward = allocate_reward(action, mu)\n",
    "        Q, delta = update_q(Q, action, reward, alpha)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action) \n",
    "        deltas.append(delta)\n",
    "\n",
    "    return actions, rewards, Qs, deltas\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Explore the parameters\n",
    "We already wrote you a little function that simply uses your `simulate_M3RescorlaWagner_v1` function to simulate an experiment based on your parameters and plots its outputs in a figure. Below, we will go through some parameter combinations and explore their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_rescorla_game(T, mu, alpha, beta):\n",
    "    fig, ax = plt.subplots(1,1, figsize = (16,8))\n",
    "    actions, rewards, Qs, deltas = simulate_M3RescorlaWagner_v1(T, mu, alpha, beta)\n",
    "    df = pd.DataFrame(Qs)\n",
    "    df['pe'] = pd.Series(deltas, name = 'delta')\n",
    "    df.columns = ['Q1','Q2','pe']\n",
    "    df.pe.plot(alpha = .5)\n",
    "    df.Q1.plot()\n",
    "    df.Q2.plot()\n",
    "    sns.despine()\n",
    "    plt.legend()\n",
    "    ax.set_ylim([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some example parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .2 # learning rate\n",
    "beta = 4 # inverse temperature\n",
    "plot_rescorla_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reducing the learning rate (participant does not manage to learn probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .01 # learning rate\n",
    "beta = 4 # inverse temperature\n",
    "plot_rescorla_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's give them more trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .01 # learning rate\n",
    "beta = 4 # inverse temperature\n",
    "plot_rescorla_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An extreme beta value makes participants stop exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .1 # learning rate\n",
    "beta = 1000 # inverse temperature\n",
    "plot_rescorla_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If beta is very low, participants still learn, but they don't use that knowledge (something that we cannot see in this plot yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .1 # learning rate\n",
    "beta = .0000000001 # inverse temperature\n",
    "plot_rescorla_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Choice Kernel\n",
    "The only difference between choice kernel and rescorla-wagner is that the softmax function takes choice_kernel values instead of Q-values as inputs and the update function is indendent of rewards.  Therefore, we only need to change the update function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Write a new update function\n",
    "The formula of the update function looks like this:\n",
    "\n",
    "$CK_{a,t+1} = CK_{a,t} + \\alpha * 1$\n",
    "\n",
    "Before the choice kernel is updated, both choice kernels decay with the inverse of the learning rate:\n",
    "\n",
    "$CK_{t+1} = CK_{t} * (1 - \\alpha) $\n",
    "\n",
    "\n",
    "Call this function `update_choice_kernels`, with the inputs `choice_kernel`, `action`, and `alpha`.\n",
    "\n",
    "> Note: If you're confused by the multiplication with one: This is only for comparability with the Rescorla-Wagner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_choice_kernels(choice_kernel, action, alpha):\n",
    "    choice_kernel = (1 - alpha) * choice_kernel\n",
    "    choice_kernel[action] = choice_kernel[action] + alpha * 1\n",
    "    return choice_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Putting it all together.\n",
    "Now make a `simulate_M4ChoiceKernel_v1` function similar to the rescorla wagner function but with our new update function and choice kernels instead of Qs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def simulate_M4ChoiceKernel_v1(T, mu, alpha, beta):\n",
    "    choice_kernel = np.array([0, 0])\n",
    "    choice_kernels = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    for t in range(T): \n",
    "        choice_kernels.append(choice_kernel)        \n",
    "        p = softmax(choice_kernel, beta)        \n",
    "        action = choose(p) \n",
    "        reward = allocate_reward(action, mu)\n",
    "        actions.append(action) \n",
    "        rewards.append(reward)        \n",
    "        choice_kernel = update_choice_kernels(choice_kernel, action, alpha)\n",
    "\n",
    "        \n",
    "        \n",
    "    return actions, rewards, choice_kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that choice_kernel decays with the inverted alpha in each trial `choice_kernel = (1 - alpha_c) * choice_kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Explore the parameters of your choice kernel simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_choice_kernel_game(T, mu, alpha, beta):\n",
    "    fig, ax = plt.subplots(1,1, figsize = (16,8))\n",
    "    actions, rewards, Cks = simulate_M4ChoiceKernel_v1(T, mu, alpha, beta)\n",
    "    df = pd.DataFrame(Cks)\n",
    "    df.columns = ['Ck1','Ck2']\n",
    "    df.Ck1.plot()\n",
    "    df.Ck2.plot()\n",
    "    sns.despine()\n",
    "    plt.legend()\n",
    "    ax.set_ylim([-.2,1.1])\n",
    "    \n",
    "    # Adding actions\n",
    "    action_df = pd.DataFrame({'action':actions})\n",
    "    action_df['trial'] = action_df.index\n",
    "    action_df['y'] = -.1\n",
    "    action_df['action'] = action_df.action.replace({0:'1',1:'2'})\n",
    "    sns.scatterplot(x = 'trial', y = 'y', hue = 'action', data = action_df, ax = ax, hue_order = ['1','2'])\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .2 # learning rate\n",
    "beta = 4 # inverse temperature\n",
    "actions = plot_choice_kernel_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .2 # learning rate\n",
    "beta = .004 # inverse temperature\n",
    "actions = plot_choice_kernel_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 500 # number of trials\n",
    "mu = [.2,.8] # reward probabilities\n",
    "alpha = .2 # learning rate\n",
    "beta = 4 # inverse temperature\n",
    "actions = plot_choice_kernel_game(T, mu, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With a reasonably high beta one of the options should always move towards 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Collins' figures\n",
    "![alt text](images/wilson_figure.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: Reproducing Panel A\n",
    "Panel A shows the different in the probability of sticking to the same action (y-axis) depending on whether the last trial was a win or a loss (x-axis), split up by learning model (colors).  Here, we will only focus on the Rescorla-Wagner and the Choice Kernel model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Write a fuction that first defines last_action and last_reward.  Then determines whether a trial was a stay or switch trial.  Finally it calculates the proportion of stay trials depending on whether the last trial was rewarded or not rewarded and outputs the score for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can use `df.action.shift(1)` to create variables that represent the value of the last row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_WSLS_v1(df):\n",
    "    # Shifting data back by one\n",
    "    df['last_action'] = df.action.shift(1)\n",
    "    df['last_reward'] = df.reward.shift(1)\n",
    "    # Deciding whether trial is stay trial\n",
    "    df['stay'] = (df.action == df.last_action).astype(int)\n",
    "    # Calculating mean stay by case and outputing as series\n",
    "    output = df.groupby(['last_reward']).stay.mean()\n",
    "    loseStay = output.loc[0]\n",
    "    winStay = output.loc[1]\n",
    "    s = pd.Series([loseStay, winStay])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Simulate data for 110 participants using the Rescorla-Wagner model and store their proportion of stay trials depending on the last trial's reward in a dataframe.  Finally, you calculate the mean proportion of stay trials (for wins and losses) over all participants (call the variable `rw`), which we will use in the plot.\n",
    "\n",
    "For this, we will use the same experiment and participant parameters as Collins, below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100 \n",
    "mu = [.2,.8]\n",
    "alpha = .1\n",
    "beta = 5\n",
    "participants = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: To turn the actions and rewards into a dataframe, you can use `df = pd.DataFrame({'action':actions, 'reward':rewards})`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(participants):\n",
    "    actions, rewards, Cks, _ = simulate_M3RescorlaWagner_v1(T, mu, alpha, beta)\n",
    "    df = pd.DataFrame({'action':actions, 'reward':rewards})\n",
    "    data.append(analysis_WSLS_v1(df))\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['loseStay','winStay']\n",
    "rw = df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw.plot()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if this worked, let's plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Now do the same for the choice kernel and call the variable `ck`.  Note that for this, Collins changed beta to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 3 # inverse temperature\n",
    "data = []\n",
    "for i in range(participants):\n",
    "    actions, rewards, Cks = simulate_M4ChoiceKernel_v1(T, mu, alpha, beta)\n",
    "    df = pd.DataFrame({'action':actions, 'reward':rewards})\n",
    "    data.append(analysis_WSLS_v1(df))\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['loseStay','winStay']\n",
    "ck = df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "We plot everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rw.plot()\n",
    "ax = ck.plot(ax = ax)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 7: Reproducing Panel B\n",
    "To create Panel B, we simulate the proportion of correct choices (y-axis) split by different values of alpha (x-axis) and beta (colors), split by early (first ten) and late (last ten) trials (subpanels).  These are the alphas and betas that Collins uses for this plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = list(np.arange(.02, 1.02, .02))\n",
    "betas = [1,2,5,10,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Write a nested loop that loops through 200 simulations, all alphas, and all betas.  In each iteration it simulates data using the Rescorla-Wagner model and outputs the proportion of correct choices (a choice is correct when the stimulus that is more likely to return a reward is picked) in the first and last ten trials.\n",
    "\n",
    "> Note: To store the output first make a `data` list and in each iteration make a `session` dictionary which you append to the data list.\n",
    "\n",
    "> Note: To get the correct action (i.e. the action associated with the highest reward probability, us `np.argmax(mu)`.\n",
    "\n",
    "> Note: Collins runs 1000 simulations, but because our code runs a bit slow, we recommend doing this 200 per alpha beta combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = []\n",
    "for i in range(200):\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            a, r, _, _ = simulate_M3RescorlaWagner_v1(T, mu, alpha, beta)\n",
    "            session_dict = {}\n",
    "            session_dict['alpha'] = alpha\n",
    "            session_dict['beta'] = beta\n",
    "            \n",
    "            imax = np.argmax(mu) # Mu max index\n",
    "            a = pd.Series(a)\n",
    "            session_dict['correct_early'] = (a.iloc[:10] == imax).mean()\n",
    "            session_dict['correct_late'] = (a.iloc[-10:] == imax).mean()\n",
    "            data.append(session_dict)\n",
    "            \n",
    "df = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/wilson_figure.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 \n",
    "Let's plot this all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: You can use `fig, axs = plt.subplots(1,2, figsize = (14,7))` to make a figure with several plots in it; you can use `sns.lineplot` to make nice lineplots; this also lets you chose the color palette (we already chose `sns.color_palette(\"rocket_r\")[:5]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"rocket_r\")[:5]\n",
    "fig, axs = plt.subplots(1,2, figsize = (14,7))\n",
    "\n",
    "sns.lineplot(x = 'alpha', y = 'correct_early', hue = 'beta', ci = None, data = df, ax = axs[0], legend = True, palette = palette)\n",
    "#\n",
    "\n",
    "sns.lineplot(x = 'alpha', y = 'correct_late', hue = 'beta', ci = None, data = df, ax = axs[1], palette = palette, legend = False)\n",
    "axs[0].legend(loc='upper right', bbox_to_anchor=(1.1, 1))\n",
    "axs[0].set_title(\"early trials\")\n",
    "axs[1].set_title(\"late trials\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well done! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Combine all plots to make them look exactly as Collins'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Enter your code here:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Check the solution below:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No solution, you're on your own :p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
