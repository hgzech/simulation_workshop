{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b8d43-60fe-4c5e-84a9-59e77482f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.foundation import patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0427dd-57e2-4658-adbf-9d2a113f48a7",
   "metadata": {},
   "source": [
    "## A (very) simple Person.\n",
    "Let's create people that can say their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee42086-078a-494c-9a1a-a73623caa307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def say_hi(self):\n",
    "        print(\"Hi, I'm %s!\"%self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a41046-ba19-432d-ba28-a1c2146033b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paul = Person(\"Paul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d1238-a736-4831-a269-ab6f4a4cf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "marie = Person(\"Marie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eac37f-ed5a-4c03-ba98-91c9cdcdab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Paul!\n"
     ]
    }
   ],
   "source": [
    "paul.say_hi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402065d0-b9cd-4e65-a29e-bdd2fc23a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Marie!\n"
     ]
    }
   ],
   "source": [
    "marie.say_hi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856971e-e361-4031-80b3-2f622e132736",
   "metadata": {},
   "source": [
    "## Our first \"learning\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4b980-13e7-449d-9caa-6433b3131f0c",
   "metadata": {},
   "source": [
    "The random model is the easiest, it does not actually learn anything, but simply gives us equal choice probabilities for each possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d110325-3c4a-44f6-98b9-e69ba1e2b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomModel():\n",
    "    def __init__(self):\n",
    "        self.person = None\n",
    "        pass\n",
    "    \n",
    "    def get_choice_probabilities(self, actions):\n",
    "        choice_probabilities = {}\n",
    "        for action in actions:\n",
    "            choice_probabilities[action] = 1/len(actions)\n",
    "        \n",
    "        return choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55151771-fc31-42ab-94be-6d8899d86b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.5, 'b': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando = RandomModel()\n",
    "possible_actions = ['a','b']\n",
    "rando.get_choice_probabilities(possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4831c-838b-4cb0-980c-bd36bc4c16a7",
   "metadata": {},
   "source": [
    "Let's allow people to have learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f4bbb-d3b4-4fbf-ab85-4ef28cd3fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def set_learning_model(self:Person, model):\n",
    "    self.learning_model = model\n",
    "    model.person = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c29f4b-3ac5-4fee-884a-283a526ce95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.RandomModel"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paul.set_learning_model(RandomModel())\n",
    "type(paul.learning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff840712-3ba7-4f7a-8aba-15072abb997d",
   "metadata": {},
   "source": [
    "And to make choices based on these learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af588659-57cd-4e39-baef-3f1153ead3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def choose_action(self:Person, possible_actions):\n",
    "    '''Note: This only works for two actions'''\n",
    "    # Getting choice probabilities from learning model\n",
    "    choice_probabilities = self.learning_model.get_choice_probabilities(possible_actions)\n",
    "    # Chosing action based on choice probabilities\n",
    "    random = np.random.random()\n",
    "    if random < list(choice_probabilities.values())[0]:\n",
    "        chosen_action = list(choice_probabilities.keys())[0]\n",
    "    else:\n",
    "        chosen_action = list(choice_probabilities.keys())[1]\n",
    "    return chosen_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e335f-73ea-49ad-9e78-7e67875476e3",
   "metadata": {},
   "source": [
    "> Warning: This choice function only works for two actions.  Are there functions that work for more actions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4d89b-80ec-4981-a1f8-c0b034f7a2ac",
   "metadata": {},
   "source": [
    "Paul chooses an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bda60-0a67-452b-af7d-4ede23cedf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paul.choose_action(['a','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8ac6c-3508-4db9-a037-84299621abf2",
   "metadata": {},
   "source": [
    "Does Paul choose randomely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c941ac7-e96c-4e1a-8825-107b6775bd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([paul.choose_action(['a','b']) == 'a' for i in range(1000)]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1101470-d48b-47ec-84a2-287f7e5a78be",
   "metadata": {},
   "source": [
    "Yes! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a2c17-49cf-4251-b439-0b7c935e3033",
   "metadata": {},
   "source": [
    "## A more complicated model\n",
    "### RescorlaWagnerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79872f7-0fd9-4695-bd8b-5798b1f9db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagnerModel():\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.person = None\n",
    "        self.alpha = alpha # A Rescorla-Wagner Model has a learning rate...\n",
    "        self.beta = beta # ...and an inverse temperature parameter\n",
    "        self.expected_reward_memory = {} # It can memorize expected rewards but starts with no knowledge of the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c83a9e-5a66-4d30-bad3-d5562ef560aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resco = RescorlaWagnerModel(alpha = .2, beta = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b6832-b87f-47c4-badf-602312a92852",
   "metadata": {},
   "source": [
    "> A Rescorla-Wagner Model associates each action with an expected reward.  If it has not encountered a possible action, it assignes .5 to it (Note: Again this only seems to work for two actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df331742-47c2-4d08-b3a9-5f966fb8ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_expected_reward_for_action(self:RescorlaWagnerModel, action):\n",
    "    # If we haven't encountered the action, we set its expected reward to .5 and remember it\n",
    "    if action not in self.expected_reward_memory:\n",
    "        self.expected_reward_memory[action] = .5\n",
    "    # We return the expected reward associated with the action\n",
    "    return self.expected_reward_memory[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c01b1d-0b72-44de-b968-c1ce43b7e033",
   "metadata": {},
   "source": [
    "The model starts with no knowledge of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dae58c-a1f1-464a-b672-b7e3a239b4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.expected_reward_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7501d-50d3-4a35-a31d-0f2c4f5399d1",
   "metadata": {},
   "source": [
    "It encounteres an action and sets it's expected reward to .5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db1f80-d06b-4a43-8732-2ad60862c6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.get_expected_reward_for_action('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fab13-26aa-4412-a7ad-100e2b8c0684",
   "metadata": {},
   "source": [
    "After encountering an action, the model remembers it's reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c61ff8-f11a-4f9b-ae1e-26c818fce47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.expected_reward_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e43065-9b6e-434e-a7c5-03e1596706a0",
   "metadata": {},
   "source": [
    "Let's write a function that can consider several actions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eab76-67cb-46c2-a3a3-0ef54712386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_expected_rewards_for_possible_actions(self:RescorlaWagnerModel, actions):\n",
    "    expected_rewards = {}\n",
    "    for action in actions:\n",
    "        expected_rewards[action] = self.get_expected_reward_for_action(action)\n",
    "    return expected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94454328-33f1-43e7-bb4e-bc04f4efa7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0.5, 'c': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.get_expected_rewards_for_possible_actions(['b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03556df-d539-4ecb-9e95-0b2218523103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.5, 'b': 0.5, 'c': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.expected_reward_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609617d1-ab95-461b-b7cd-0c14369fac82",
   "metadata": {},
   "source": [
    "Based on the expected rewards, we can write the models choice function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18728e-b9b5-4fc8-88b2-5a7d98490109",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_choice_probabilities(self:RescorlaWagnerModel, actions):\n",
    "    expected_rewards = self.get_expected_rewards_for_possible_actions(actions)\n",
    "    expected_reward_values = np.array(list(expected_rewards.values()))\n",
    "    choice_probabilities = np.exp(expected_reward_values * self.beta) / sum(np.exp(expected_reward_values * self.beta))\n",
    "    choice_probabilities = dict(zip(actions,choice_probabilities)) # turning them into a dictionary\n",
    "    return choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63477c75-a808-41d8-9b67-19703b46a8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0.5, 'c': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.get_choice_probabilities(['b','c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e10f0-f713-4d9f-9df6-09f7a0086dbb",
   "metadata": {},
   "source": [
    "Now a person with a Rescorla Wagner Model should already be able to choose things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3b378-f227-4e4b-b870-917926237028",
   "metadata": {},
   "outputs": [],
   "source": [
    "richard = Person('Richard')\n",
    "richard.set_learning_model(RescorlaWagnerModel(alpha = .2, beta = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246529f-8ce5-4705-962f-1553ae067d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard.choose_action(['a','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96efc76-ff84-47af-bb58-ce19fa56a73b",
   "metadata": {},
   "source": [
    "At this point the Rescorla Wagner Model acts the same as the random model.  This is because we have not given it the ability to learn (update it's expected reward values).  Therefore the values stay at their initial value .5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ea278-9e64-4aa8-9271-691f8f937fcb",
   "metadata": {},
   "source": [
    "Let's give the model the chance to learn by associating actions with rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c013f0c-b1f1-4063-b06c-da09728a5c68",
   "metadata": {},
   "source": [
    "> Warning: For some reason rewards have to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ccfad-ccc8-4b8b-bc9d-494ac2cbedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def learn(self:RescorlaWagnerModel, action, reward):\n",
    "    self.prediction_error = reward - self.get_expected_reward_for_action(action)\n",
    "    self.expected_reward_memory[action] = self.expected_reward_memory[action] + self.alpha * self.prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb7776-509c-4945-9702-1d5c401ef5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.5, 'b': 0.5, 'c': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.expected_reward_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b4e0c-aa38-4ba5-96b3-5a286b710f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resco.learn('a',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc22265-1d49-4cbe-831b-6f23d0b9f3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.6, 'b': 0.5, 'c': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resco.expected_reward_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb3993-a581-4177-bddc-0692a213f4eb",
   "metadata": {},
   "source": [
    "For learning to occur, of course, the participant has to remember her last action (at least until receiving a reward):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9f3db-a7df-4f68-8207-e4017d97d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def choose_and_remember_action(self:Person, possible_actions):\n",
    "    action = self.choose_action(possible_actions)\n",
    "    self.last_action = action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654e4f2-268f-4a6e-b23e-6e7bcf83aa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard.choose_and_remember_action(['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bfe8f-14bb-44a6-94d3-fd09aefc6d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard.last_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5170874-8c2f-4013-8709-b2bd22e5cf4d",
   "metadata": {},
   "source": [
    "Now when the person gets rewarded, the model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4c18b-05ee-4292-9e2e-323556b7574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_rewarded(self:Person, reward):\n",
    "    self.learning_model.learn(self.last_action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d628632-9ff5-44b7-a4f9-ab9455c7271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "richard.get_rewarded(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e741d88-633d-4124-b031-48cfd8873a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.5, 'b': 0.6}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard.learning_model.expected_reward_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4709f7-dbe2-4f95-83fa-3c4b8af61cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard.choose_and_remember_action(['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9347202-69a4-44aa-beae-d11a32e60940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    richard.get_rewarded(1) # Jackpot!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60485d-bfd8-4dcc-89a0-4e0aa57e8f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.9999999999999998, 'b': 0.6}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard.learning_model.expected_reward_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4de49-32ad-48d2-86cf-e147da95adc8",
   "metadata": {},
   "source": [
    "Richard should be pretty fond of a now.  Note that he still chooses a only in about 80% of the cases.  Could you imagine why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa132b-5ca5-4fde-b7c2-8399d7afc625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([richard.choose_action(['a','b']) == 'a' for i in range(1000)]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
